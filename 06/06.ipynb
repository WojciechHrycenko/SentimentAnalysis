{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5af06b94",
   "metadata": {},
   "source": [
    "<div style=\"background-color:white; text-align:center; font-family:Arial, Helvetica, sans-serif; padding:50px;\">\n",
    "  <!-- Tytuł -->\n",
    "  <div style=\"color:#993520; font-size:60px; font-weight:bold; margin-bottom:20px;\">\n",
    "    SENTIMENT ANALYSIS\n",
    "  </div>\n",
    "\n",
    "  <!-- Podtytuł -->\n",
    "  <div style=\"color:#993520; font-size:35px; margin-bottom:40px;\">\n",
    "    Supervised sentiment analysis: feature extraction & selection\n",
    "  </div>\n",
    "\n",
    "  <!-- Autor -->\n",
    "  <div style=\"color:black; font-size:30px; margin-bottom:10px;\">\n",
    "    Maciej Świtała, PhD\n",
    "  </div>\n",
    "\n",
    "  <!-- Data / semestr -->\n",
    "  <div style=\"color:black; font-size:30px; margin-bottom:50px;\">\n",
    "    Autumn 2025\n",
    "  </div>\n",
    "\n",
    "  <!-- Logo -->\n",
    "  <div>\n",
    "    <img src=\"img/wne-logo-new-en.jpg\" alt=\"WNE Logo\" style=\"max-width:400px; height:auto;\">\n",
    "  </div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4005ec",
   "metadata": {},
   "source": [
    "### 1. Intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "970fc09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas numpy matplotlib nltk scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9423b673",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  # for working with data in DataFrames\n",
    "import numpy as np  # numerical operations and arrays\n",
    "\n",
    "import matplotlib.pyplot as plt  # data visualization\n",
    "\n",
    "import pickle  # data loading\n",
    "import math  # mathematical functions\n",
    "import ast\n",
    "\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "from collections import Counter  # counting occurrences of elements\n",
    "from itertools import islice\n",
    "\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer # VADER algorithm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, GridSearchCV\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import make_scorer, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62d6e6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us load an example dataset (already initially preprocessed); these are opinions on an individual McDonald's restaurant in the U.S.\n",
    "data = pd.read_csv(\"data/nichecom-opinions-mcdonalds_cleaned.txt\", sep=\"\\t\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1352a9f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Position</th>\n",
       "      <th>Date</th>\n",
       "      <th>Review_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>working for McDonald's is very unique you lear...</td>\n",
       "      <td>Rating 4 out of 5</td>\n",
       "      <td>Senior Employee</td>\n",
       "      <td>2024-09-30</td>\n",
       "      <td>['working', 'mcdonalds', 'unique', 'learn', 'l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It have been great so far. The people are real...</td>\n",
       "      <td>Rating 3 out of 5</td>\n",
       "      <td>Entry Level Employee</td>\n",
       "      <td>2024-09-12</td>\n",
       "      <td>['great', 'far', 'people', 'really', 'kind', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Mcdonalds that I work at is a very good jo...</td>\n",
       "      <td>Rating 3 out of 5</td>\n",
       "      <td>Manager / Director</td>\n",
       "      <td>2024-09-09</td>\n",
       "      <td>['mcdonalds', 'work', 'good', 'job', 'problem'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>While the workplace environment may not be per...</td>\n",
       "      <td>Rating 2 out of 5</td>\n",
       "      <td>Intern / Student Worker</td>\n",
       "      <td>2024-06-10</td>\n",
       "      <td>['workplace', 'environment', 'may', 'perfect',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The overall experience of working at a McDonal...</td>\n",
       "      <td>Rating 3 out of 5</td>\n",
       "      <td>Other</td>\n",
       "      <td>2024-05-31</td>\n",
       "      <td>['overall', 'experience', 'working', 'mcdonald...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review             Rating  \\\n",
       "0  working for McDonald's is very unique you lear...  Rating 4 out of 5   \n",
       "1  It have been great so far. The people are real...  Rating 3 out of 5   \n",
       "2  The Mcdonalds that I work at is a very good jo...  Rating 3 out of 5   \n",
       "3  While the workplace environment may not be per...  Rating 2 out of 5   \n",
       "4  The overall experience of working at a McDonal...  Rating 3 out of 5   \n",
       "\n",
       "                  Position        Date  \\\n",
       "0          Senior Employee  2024-09-30   \n",
       "1     Entry Level Employee  2024-09-12   \n",
       "2       Manager / Director  2024-09-09   \n",
       "3  Intern / Student Worker  2024-06-10   \n",
       "4                    Other  2024-05-31   \n",
       "\n",
       "                                      Review_cleaned  \n",
       "0  ['working', 'mcdonalds', 'unique', 'learn', 'l...  \n",
       "1  ['great', 'far', 'people', 'really', 'kind', '...  \n",
       "2  ['mcdonalds', 'work', 'good', 'job', 'problem'...  \n",
       "3  ['workplace', 'environment', 'may', 'perfect',...  \n",
       "4  ['overall', 'experience', 'working', 'mcdonald...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f842c648",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(472, 5)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad73af69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us simulate that we do have a continuous dependent variable resembling sentiment\n",
    "# for educational purposes it can be a compound score obtained from VADER algorithm\n",
    "def vader(text):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    sentiment_scores = sia.polarity_scores(text)\n",
    "    return sentiment_scores['compound']\n",
    "\n",
    "data['y_continuous'] = data['Review'].apply(vader)\n",
    "\n",
    "# another option would be considering a binary classification with 1 for, e.g., highest sentiment score\n",
    "# and 0 otherwise; for educational purposes let us create such a target variable\n",
    "data['y_binary'] = (data['Rating']=='Rating 5 out of 5').astype(int)\n",
    "\n",
    "# however, in our example it would be most reasonable to consider multiclassification problem with target:\n",
    "data['y_multiclass'] = pd.to_numeric(data['Rating'].map(lambda x: x.split('Rating ')[1].split(' out of ')[0]))\n",
    "\n",
    "# we will proceed further, preparing features for all three possible target variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9022cac",
   "metadata": {},
   "source": [
    "### 2. Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cedc15c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function generates n-grams for a list of tokens over a specified range of n values\n",
    "def generate_ngrams(tokens, ngram_range=(1,3)):\n",
    "\n",
    "    all_ngrams = []  # this list will store all generated n-grams\n",
    "\n",
    "    # loop over each n in the range (e.g., 1, 2, 3)\n",
    "    for n in range(ngram_range[0], ngram_range[1]+1):\n",
    "        # create sliding windows of length n\n",
    "        ngrams = zip(*[tokens[i:] for i in range(n)])  # e.g., for bigrams: [tokens[0:], tokens[1:]]\n",
    "        \n",
    "        # join tokens with underscores to form n-gram strings\n",
    "        all_ngrams.extend(['_'.join(gram) for gram in ngrams])\n",
    "    \n",
    "    return all_ngrams  # return the complete list of n-grams\n",
    "\n",
    "# apply n-gram generation to all cleaned texts\n",
    "# texts_cleaned is a list of documents, each a list of tokens\n",
    "texts_with_ngrams = [generate_ngrams(doc, ngram_range=(1,3)) for doc in data['Review_cleaned'].apply(ast.literal_eval)]\n",
    "\n",
    "# after this step:\n",
    "# - texts_with_ngrams[i] contains all unigrams, bigrams, and trigrams for document i\n",
    "# - each n-gram is a single string (e.g., \"new_york\", \"war_in_ukraine\")\n",
    "\n",
    "# show a sample of cleaned texts\n",
    "# print(texts_with_ngrams[0])\n",
    "\n",
    "# count unique tokens after incorporation of n-grams\n",
    "freq_dict = Counter(token for doc in texts_with_ngrams for token in np.unique(doc))\n",
    "# print('there are', len(freq_dict), 'unique tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e433a1",
   "metadata": {},
   "source": [
    "### 3. Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d4322f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n. obs. in train: 377\n",
      "n. obs. in test: 95\n",
      "\n",
      "y_train:\n",
      "count    377.000000\n",
      "mean       0.537701\n",
      "std        0.535235\n",
      "min       -0.978200\n",
      "25%        0.361200\n",
      "50%        0.811500\n",
      "75%        0.900100\n",
      "max        0.987800\n",
      "Name: y_continuous, dtype: float64\n",
      "\n",
      "y_test:\n",
      "count    95.000000\n",
      "mean      0.499582\n",
      "std       0.560678\n",
      "min      -0.920600\n",
      "25%       0.366400\n",
      "50%       0.765000\n",
      "75%       0.892700\n",
      "max       0.968200\n",
      "Name: y_continuous, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# \"stratified\" split for continuous target variable\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(\n",
    "    texts_with_ngrams, data['y_continuous'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print('n. obs. in train:',len(X_train1))\n",
    "print('n. obs. in test:',len(X_test1))\n",
    "\n",
    "print('\\ny_train:\\n',y_train1.describe(),sep='')\n",
    "print('\\ny_test:\\n',y_test1.describe(),sep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a3a0944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n. obs. in train: 377\n",
      "n. obs. in test: 95\n",
      "\n",
      "\n",
      "1s % in y_train: 0.2838\n",
      "1s % in y_test: 0.2842\n"
     ]
    }
   ],
   "source": [
    "# stratified split for binary target variable\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(\n",
    "    texts_with_ngrams, data['y_binary'], test_size=0.2, random_state=42, stratify=data['y_binary']\n",
    ")\n",
    "\n",
    "print('n. obs. in train:',len(X_train2))\n",
    "print('n. obs. in test:',len(X_test2))\n",
    "print('\\n')\n",
    "print('1s % in y_train:',np.round((y_train2==1).sum()/len(y_train2),4))\n",
    "print('1s % in y_test:',np.round((y_test2==1).sum()/len(y_test2),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ffafbc2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n. obs. in train: 377\n",
      "n. obs. in test: 95\n",
      "\n",
      " y_multiclass\n",
      "4    0.3024\n",
      "3    0.2997\n",
      "5    0.2838\n",
      "2    0.0690\n",
      "1    0.0451\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      " y_multiclass\n",
      "4    0.3053\n",
      "3    0.2947\n",
      "5    0.2842\n",
      "2    0.0737\n",
      "1    0.0421\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# stratified split for multiclass target variable\n",
    "X_train3, X_test3, y_train3, y_test3 = train_test_split(\n",
    "    texts_with_ngrams, data['y_multiclass'], test_size=0.2, random_state=42, stratify=data['y_multiclass']\n",
    ")\n",
    "\n",
    "print('n. obs. in train:',len(X_train3))\n",
    "print('n. obs. in test:',len(X_test3))\n",
    "\n",
    "print('\\n',np.round(y_train3.value_counts(normalize=True), 4))\n",
    "print('\\n',np.round(y_test3.value_counts(normalize=True), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26c391e",
   "metadata": {},
   "source": [
    "### 4. Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8bed2ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function computes TF, DF, IDF, TF-IDF for all the tokens under consideration\n",
    "\n",
    "def compute_tfidf(texts):\n",
    "\n",
    "    N = len(texts)  # number of documents\n",
    "    \n",
    "    # 1. Compute TF for each document\n",
    "    tf_docs = []\n",
    "    for doc in texts:\n",
    "        counts = Counter(doc)\n",
    "        total_words = len(doc)\n",
    "        tf_doc = {word: count / total_words for word, count in counts.items()}\n",
    "        tf_docs.append(tf_doc)\n",
    "    \n",
    "    # 2. Compute DF for each token (fraction of docs containing the token)\n",
    "    df_dict = {}\n",
    "    for doc in texts:\n",
    "        for token in set(doc):\n",
    "            df_dict[token] = df_dict.get(token, 0) + 1\n",
    "    df_dict = {token: count / N for token, count in df_dict.items()}\n",
    "    \n",
    "    # 3. Compute IDF\n",
    "    idf_dict = {token: math.log(1 / df) for token, df in df_dict.items()}\n",
    "    \n",
    "    # 4. Compute TF-IDF for each document\n",
    "    tfidf_docs = []\n",
    "    for tf_doc in tf_docs:\n",
    "        tfidf_doc = {token: tf_val * idf_dict[token] for token, tf_val in tf_doc.items()}\n",
    "        tfidf_docs.append(tfidf_doc)\n",
    "    \n",
    "    return tf_docs, df_dict, idf_dict, tfidf_docs\n",
    "\n",
    "tf_docs1, df_dict1, idf_dict1, tfidf_docs1 = compute_tfidf(X_train1)\n",
    "tf_docs2, df_dict2, idf_dict2, tfidf_docs2 = compute_tfidf(X_train2)\n",
    "tf_docs3, df_dict3, idf_dict3, tfidf_docs3 = compute_tfidf(X_train3)\n",
    "\n",
    "# usually we are interested in DF, primarily due to its intuitive interpretation\n",
    "# initial analysis of DF is the first step towards filtering out the too frequent and too rare tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91c344a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering both subsets with respect to statistics obtained for the training subsets\n",
    "df_dict1_filtered = {key: value for key, value in df_dict1.items() if (value > 2/len(X_train1)) & (value <= 0.95)}\n",
    "allowed_keys = set(df_dict1_filtered.keys())\n",
    "X_train1_filtered = [[word for word in row if word in allowed_keys] for row in X_train1]\n",
    "X_test1_filtered  = [[word for word in row if word in allowed_keys] for row in X_test1]\n",
    "\n",
    "df_dict2_filtered = {key: value for key, value in df_dict2.items() if (value > 2/len(X_train2)) & (value <= 0.95)}\n",
    "allowed_keys = set(df_dict2_filtered.keys())\n",
    "X_train2_filtered = [[word for word in row if word in allowed_keys] for row in X_train2]\n",
    "X_test2_filtered  = [[word for word in row if word in allowed_keys] for row in X_test2]\n",
    "\n",
    "df_dict3_filtered = {key: value for key, value in df_dict3.items() if (value > 2/len(X_train3)) & (value <= 0.95)}\n",
    "allowed_keys = set(df_dict3_filtered.keys())\n",
    "X_train3_filtered = [[word for word in row if word in allowed_keys] for row in X_train3]\n",
    "X_test3_filtered  = [[word for word in row if word in allowed_keys] for row in X_test3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93099d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting tokens in documents\n",
    "def count_words(doc):\n",
    "    return dict(Counter(doc))\n",
    "\n",
    "# token counts for training subset\n",
    "token_counts_train_list1 = [count_words(doc) for doc in X_train1]\n",
    "token_counts_train_df1 = pd.DataFrame(token_counts_train_list1).fillna(0).astype(int)\n",
    "# token counts for test subset; tylko tokeny obecne w treningu\n",
    "token_counts_test_list1 = []\n",
    "allowed_tokens = set(token_counts_train_df1.columns)\n",
    "for doc in X_test1:\n",
    "    counts = count_words(doc)\n",
    "    filtered_counts = {token: count for token, count in counts.items() if token in allowed_tokens}\n",
    "    token_counts_test_list1.append(filtered_counts)\n",
    "token_counts_test_df1 = pd.DataFrame(token_counts_test_list1).reindex(columns=token_counts_train_df1.columns, fill_value=0).fillna(0).astype(int)\n",
    "\n",
    "# token counts for training subset\n",
    "token_counts_train_list2 = [count_words(doc) for doc in X_train2]\n",
    "token_counts_train_df2 = pd.DataFrame(token_counts_train_list2).fillna(0).astype(int)\n",
    "# token counts for test subset; tylko tokeny obecne w treningu\n",
    "token_counts_test_list2 = []\n",
    "allowed_tokens = set(token_counts_train_df2.columns)\n",
    "for doc in X_test2:\n",
    "    counts = count_words(doc)\n",
    "    filtered_counts = {token: count for token, count in counts.items() if token in allowed_tokens}\n",
    "    token_counts_test_list2.append(filtered_counts)\n",
    "token_counts_test_df2 = pd.DataFrame(token_counts_test_list2).reindex(columns=token_counts_train_df2.columns, fill_value=0).fillna(0).astype(int)\n",
    "\n",
    "# token counts for training subset\n",
    "token_counts_train_list3 = [count_words(doc) for doc in X_train3]\n",
    "token_counts_train_df3 = pd.DataFrame(token_counts_train_list3).fillna(0).astype(int)\n",
    "# token counts for test subset; tylko tokeny obecne w treningu\n",
    "token_counts_test_list3 = []\n",
    "allowed_tokens = set(token_counts_train_df3.columns)\n",
    "for doc in X_test3:\n",
    "    counts = count_words(doc)\n",
    "    filtered_counts = {token: count for token, count in counts.items() if token in allowed_tokens}\n",
    "    token_counts_test_list3.append(filtered_counts)\n",
    "token_counts_test_df3 = pd.DataFrame(token_counts_test_list3).reindex(columns=token_counts_train_df3.columns, fill_value=0).fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a29f37",
   "metadata": {},
   "source": [
    "### 5. Feature selection (hybrid, i.e., mutual information + LASSO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f893a595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mutual information scores for possible features in case of continuous, binary and multiclass target variable respectively (calculation takes ~2 min.)\n",
    "mutual_information_scores1 = {column: mutual_info_score(pd.cut(token_counts_train_df1[column], bins=10, labels=False), pd.cut(y_train1, bins=10, labels=False)) for column in token_counts_train_df1.columns}\n",
    "mutual_information_scores2 = {column: mutual_info_score(pd.cut(token_counts_train_df2[column], bins=10, labels=False), pd.cut(y_train2, bins=10, labels=False)) for column in token_counts_train_df2.columns}\n",
    "mutual_information_scores3 = {column: mutual_info_score(pd.cut(token_counts_train_df3[column], bins=10, labels=False), pd.cut(y_train3, bins=10, labels=False)) for column in token_counts_train_df3.columns}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44a3f8de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'work': 0.06852023599789789,\n",
       " 'poor': 0.06538583450799125,\n",
       " 'great': 0.05809834324420729,\n",
       " 'manager': 0.0520763279497507,\n",
       " 'working': 0.05174287752679016,\n",
       " 'mcdonalds': 0.04718376837014386,\n",
       " 'employee': 0.04351394327788066,\n",
       " 'hour': 0.041905322853077034,\n",
       " 'customer': 0.041344900876389666,\n",
       " 'rude': 0.041293984446295014,\n",
       " 'bit': 0.04064552968034424,\n",
       " 'okay_work': 0.03987036461559835,\n",
       " 'okay': 0.03863280088345439,\n",
       " 'job': 0.03841647681864241,\n",
       " 'good': 0.03813675637351259,\n",
       " 'mcdonalds_hour': 0.03750229360685153,\n",
       " 'hour_pay': 0.03750229360685153,\n",
       " 'pay_bit': 0.03750229360685153,\n",
       " 'bit_poor': 0.03750229360685153,\n",
       " 'okay_work_mcdonalds': 0.03750229360685153,\n",
       " 'work_mcdonalds_hour': 0.03750229360685153,\n",
       " 'mcdonalds_hour_pay': 0.03750229360685153,\n",
       " 'hour_pay_bit': 0.03750229360685153,\n",
       " 'pay_bit_poor': 0.03750229360685153,\n",
       " 'love': 0.03745569928914078}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokens with highest mutual information for continuos target variable \n",
    "mis_sorted_by_values1 = dict(sorted(mutual_information_scores1.items(), key=lambda item: item[1], reverse=True))\n",
    "dict(islice(mis_sorted_by_values1.items(), 25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c6e4928",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'great': 0.022113894664759947,\n",
       " 'awesome': 0.017781019340707898,\n",
       " 'rude': 0.01734850219884862,\n",
       " 'working': 0.014684130896618679,\n",
       " 'everything': 0.013667550138844468,\n",
       " 'atmosphere': 0.013600605969756407,\n",
       " 'employed': 0.013506858464152088,\n",
       " 'family': 0.011584618180137948,\n",
       " 'poor': 0.010831685916463454,\n",
       " 'summer': 0.010467119803805713,\n",
       " 'fast': 0.010413231552141626,\n",
       " 'hour': 0.010381145232460705,\n",
       " 'excellent': 0.010129606421450686,\n",
       " 'mcdonalds_really': 0.010102747103829912,\n",
       " 'support': 0.010102747103829912,\n",
       " 'job_flexible': 0.010102747103829912,\n",
       " 'one_work': 0.010102747103829912,\n",
       " 'great_work': 0.010102747103829912,\n",
       " 'make_job': 0.010102747103829912,\n",
       " 'make': 0.010092032007154375,\n",
       " 'good_first': 0.00991293022180137,\n",
       " 'care': 0.009300233690564416,\n",
       " 'level': 0.008997168611004237,\n",
       " 'good_first_job': 0.008997168611004233,\n",
       " 'really': 0.0085860200409387}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokens with highest mutual information for binary target variable\n",
    "mis_sorted_by_values2 = dict(sorted(mutual_information_scores2.items(), key=lambda item: item[1], reverse=True))\n",
    "dict(islice(mis_sorted_by_values2.items(), 25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8b96d3df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'employee': 0.0325730559121349,\n",
       " 'rude': 0.03210950453036741,\n",
       " 'location': 0.03174701274883544,\n",
       " 'work': 0.03148743209665761,\n",
       " 'job': 0.026249420950348837,\n",
       " 'love': 0.025900442488308154,\n",
       " 'always': 0.024039298441594064,\n",
       " 'issue': 0.023288552913919007,\n",
       " 'family': 0.02275977802203084,\n",
       " 'team': 0.022436127422461958,\n",
       " 'get': 0.02219691776184784,\n",
       " 'manager': 0.021593400894121637,\n",
       " 'could': 0.021347419754302398,\n",
       " 'customer': 0.02125308286412182,\n",
       " 'college': 0.021127843129437725,\n",
       " 'need': 0.021067981273059862,\n",
       " 'make': 0.020756891868703595,\n",
       " 'made': 0.020300429530227218,\n",
       " 'yelled': 0.02020155139116499,\n",
       " 'high': 0.01990024073743745,\n",
       " 'place': 0.019614744079676852,\n",
       " 'really': 0.01959025227053151,\n",
       " 'working': 0.019564724990008345,\n",
       " 'high_school': 0.01942974345556537,\n",
       " 'many': 0.019343980494375083}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokens with highest mutual information for multiclass target variable\n",
    "mis_sorted_by_values3 = dict(sorted(mutual_information_scores3.items(), key=lambda item: item[1], reverse=True))\n",
    "dict(islice(mis_sorted_by_values3.items(), 25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "53f29880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'feature_select__top_n': 75, 'reg__alpha': 0.01}\n",
      "Best CV RMSE: 0.4294\n",
      "Test RMSE: 0.5275\n"
     ]
    }
   ],
   "source": [
    "# custom transformer to select top-n features based on predefined order\n",
    "class TopFeatureSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, feature_order, top_n=25):\n",
    "        self.feature_order = feature_order\n",
    "        self.top_n = top_n\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        selected = list(islice(self.feature_order, self.top_n))\n",
    "        return X[selected]\n",
    "\n",
    "# predefined feature order based on importance\n",
    "feature_order = list(mis_sorted_by_values1.keys())\n",
    "\n",
    "# pipeline: feature selection -> standardization -> LASSO regression\n",
    "pipeline = Pipeline([\n",
    "    (\"feature_select\", TopFeatureSelector(feature_order=feature_order)),\n",
    "    (\"scale\", StandardScaler(with_mean=False)),\n",
    "    (\"reg\", Lasso(random_state=42, max_iter=5000))\n",
    "])\n",
    "\n",
    "# grid search parameters: number of features + regularization strength (alpha)\n",
    "param_grid = {\n",
    "    \"feature_select__top_n\": [25, 50, 75, 100, 125, 150],\n",
    "    \"reg__alpha\": [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100]\n",
    "}\n",
    "\n",
    "# k-fold cross-validation\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# scoring function: negative RMSE (GridSearchCV maksymalizuje scoring)\n",
    "rmse_scorer = make_scorer(lambda y_true, y_pred: -np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "\n",
    "# grid search\n",
    "grid = GridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    cv=cv,\n",
    "    scoring=rmse_scorer,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# fit the model on the training set\n",
    "grid.fit(token_counts_train_df1, y_train1)\n",
    "\n",
    "# print best hyperparameters and CV score\n",
    "print(\"Best params:\", grid.best_params_)\n",
    "print(\"Best CV RMSE:\", np.round(-grid.best_score_,4))\n",
    "\n",
    "# evaluate on the test set\n",
    "y_test_pred = grid.best_estimator_.predict(token_counts_test_df1)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test1, y_test_pred))\n",
    "print(\"Test RMSE:\", np.round(test_rmse,4))\n",
    "\n",
    "# the whole procedure takes ~10 s.\n",
    "# results are: RMSE (train) = 0.4294, RMSE (test) = 0.5275"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9c4f04c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitted model\n",
    "best_model = grid.best_estimator_\n",
    "\n",
    "# coefficients from multinomial lasso regression\n",
    "coefficients = best_model.named_steps['reg'].coef_\n",
    "\n",
    "# compute importance as max abs coef across classes\n",
    "importance = np.abs(coefficients)\n",
    "\n",
    "# dataframe with feature importance\n",
    "df = pd.DataFrame({\n",
    "    'feature': best_model.named_steps['feature_select'].feature_order[:best_model.named_steps['feature_select'].top_n],\n",
    "    'importance': importance\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1d2275be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>poor</td>\n",
       "      <td>0.128944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>rude</td>\n",
       "      <td>0.090439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>horrible</td>\n",
       "      <td>0.089899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>great</td>\n",
       "      <td>0.070739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>enjoy</td>\n",
       "      <td>0.069185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>mistake</td>\n",
       "      <td>0.068227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>love</td>\n",
       "      <td>0.067104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>easy</td>\n",
       "      <td>0.063245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>friendly</td>\n",
       "      <td>0.056768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>good</td>\n",
       "      <td>0.052437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>company</td>\n",
       "      <td>0.048487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>see</td>\n",
       "      <td>0.044325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>work</td>\n",
       "      <td>0.040472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>sometimes</td>\n",
       "      <td>0.040303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>manager</td>\n",
       "      <td>0.039140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>like</td>\n",
       "      <td>0.039079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>worst</td>\n",
       "      <td>0.037280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>customer</td>\n",
       "      <td>0.036885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>nice</td>\n",
       "      <td>0.036814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>experience</td>\n",
       "      <td>0.036510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>know</td>\n",
       "      <td>0.036350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>pay</td>\n",
       "      <td>0.034340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>long</td>\n",
       "      <td>0.031397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>nothing</td>\n",
       "      <td>0.030580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>benefit</td>\n",
       "      <td>0.029344</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       feature  importance\n",
       "1         poor    0.128944\n",
       "9         rude    0.090439\n",
       "27    horrible    0.089899\n",
       "2        great    0.070739\n",
       "51       enjoy    0.069185\n",
       "61     mistake    0.068227\n",
       "24        love    0.067104\n",
       "72        easy    0.063245\n",
       "32    friendly    0.056768\n",
       "14        good    0.052437\n",
       "33     company    0.048487\n",
       "43         see    0.044325\n",
       "0         work    0.040472\n",
       "42   sometimes    0.040303\n",
       "3      manager    0.039140\n",
       "29        like    0.039079\n",
       "44       worst    0.037280\n",
       "8     customer    0.036885\n",
       "68        nice    0.036814\n",
       "47  experience    0.036510\n",
       "48        know    0.036350\n",
       "74         pay    0.034340\n",
       "60        long    0.031397\n",
       "55     nothing    0.030580\n",
       "52     benefit    0.029344"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values(by='importance', ascending=False).head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0cef7c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving features with non-zero coefficients\n",
    "nonzero_features = df.loc[df['importance'] > 0,]\n",
    "\n",
    "regression_model_data = {\n",
    "    \"X_train\": token_counts_train_df1[nonzero_features['feature']],\n",
    "    \"X_test\": token_counts_test_df1[nonzero_features['feature']],\n",
    "    \"y_train\": y_train1,\n",
    "    \"y_test\": y_test1\n",
    "}\n",
    "\n",
    "# zapis do pickle\n",
    "with open(\"outputs/regression_model_data.pkl\", \"wb\") as f:\n",
    "    pickle.dump(regression_model_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "82a967c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'clf__C': 10000, 'feature_select__top_n': 125}\n",
      "Best CV AUC: 0.8167\n",
      "Test AUC: 0.6117\n"
     ]
    }
   ],
   "source": [
    "# custom transformer to select top-n features based on predefined order\n",
    "class TopFeatureSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, feature_order, top_n=25):\n",
    "        self.feature_order = feature_order\n",
    "        self.top_n = top_n\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # select the first `top_n` features from the predefined order\n",
    "        selected = list(islice(self.feature_order, self.top_n))\n",
    "        return X[selected]\n",
    "\n",
    "# predefined feature order based on importance\n",
    "feature_order = list(mis_sorted_by_values2.keys())\n",
    "\n",
    "# pipeline: feature selection -> standardization -> LASSO logistic regression\n",
    "pipeline = Pipeline([\n",
    "    (\"feature_select\", TopFeatureSelector(feature_order=feature_order)),\n",
    "    (\"scale\", StandardScaler(with_mean=False)),\n",
    "    (\"clf\", LogisticRegression(\n",
    "        penalty=\"l1\", \n",
    "        solver=\"saga\", \n",
    "        random_state=42, \n",
    "        max_iter=5000\n",
    "    ))\n",
    "])\n",
    "\n",
    "# grid search parameters: number of features + regularization strength\n",
    "param_grid = {\n",
    "    \"feature_select__top_n\": [25, 50, 75, 100, 125, 150], # try different numbers of top features\n",
    "    \"clf__C\": [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100, 1000, 10000, 100000] # regularization values for LASSO\n",
    "}\n",
    "\n",
    "# stratified k-fold cross-validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# grid search using AUC as scoring metric\n",
    "grid = GridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    cv=cv,\n",
    "    scoring=\"roc_auc\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# fit the model on the training set\n",
    "grid.fit(token_counts_train_df2, y_train2)\n",
    "\n",
    "# print best hyperparameters and CV score\n",
    "print(\"Best params:\", grid.best_params_)\n",
    "print(\"Best CV AUC:\", np.round(grid.best_score_,4))\n",
    "\n",
    "# evaluate on the test set\n",
    "y_test_pred = grid.best_estimator_.predict_proba(token_counts_test_df2)[:, 1]\n",
    "auc = roc_auc_score(y_test2, y_test_pred)\n",
    "print(\"Test AUC:\", np.round(auc,4))\n",
    "\n",
    "# the whole procedure takes ~1.5 min.\n",
    "# results are: AUC (train) = 0., AUC (test) = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fffeefb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitted model\n",
    "best_model = grid.best_estimator_\n",
    "\n",
    "# coefficients from multinomial lasso regression\n",
    "coefficients = best_model.named_steps['clf'].coef_[0]\n",
    "\n",
    "# compute importance as max abs coef across classes\n",
    "importance = np.abs(coefficients)\n",
    "\n",
    "# dataframe with feature importance\n",
    "df = pd.DataFrame({\n",
    "    'feature': best_model.named_steps['feature_select'].feature_order[:best_model.named_steps['feature_select'].top_n],\n",
    "    'importance': importance\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d555a2f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>either</td>\n",
       "      <td>1.245780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>make_job</td>\n",
       "      <td>1.155716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>make</td>\n",
       "      <td>1.090345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rude</td>\n",
       "      <td>1.056951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>much</td>\n",
       "      <td>1.023651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>hour</td>\n",
       "      <td>0.992418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>work_long</td>\n",
       "      <td>0.848031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>problem</td>\n",
       "      <td>0.830080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>two</td>\n",
       "      <td>0.829507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>atmosphere</td>\n",
       "      <td>0.818699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>good_mcdonalds</td>\n",
       "      <td>0.816578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>saving</td>\n",
       "      <td>0.815897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>go_way</td>\n",
       "      <td>0.788827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>yes</td>\n",
       "      <td>0.788299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>today</td>\n",
       "      <td>0.784330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>excellent</td>\n",
       "      <td>0.778667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>newcomer</td>\n",
       "      <td>0.756589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>employed</td>\n",
       "      <td>0.751060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>work_atmosphere</td>\n",
       "      <td>0.731379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>great_work</td>\n",
       "      <td>0.723087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>amazing_crew</td>\n",
       "      <td>0.718386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>level</td>\n",
       "      <td>0.718101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>ever</td>\n",
       "      <td>0.713511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>top</td>\n",
       "      <td>0.710720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>stress</td>\n",
       "      <td>0.703067</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             feature  importance\n",
       "28            either    1.245780\n",
       "18          make_job    1.155716\n",
       "19              make    1.090345\n",
       "2               rude    1.056951\n",
       "46              much    1.023651\n",
       "11              hour    0.992418\n",
       "74         work_long    0.848031\n",
       "122          problem    0.830080\n",
       "44               two    0.829507\n",
       "5         atmosphere    0.818699\n",
       "66    good_mcdonalds    0.816578\n",
       "104           saving    0.815897\n",
       "51            go_way    0.788827\n",
       "59               yes    0.788299\n",
       "62             today    0.784330\n",
       "12         excellent    0.778667\n",
       "48          newcomer    0.756589\n",
       "6           employed    0.751060\n",
       "72   work_atmosphere    0.731379\n",
       "17        great_work    0.723087\n",
       "98      amazing_crew    0.718386\n",
       "22             level    0.718101\n",
       "43              ever    0.713511\n",
       "121              top    0.710720\n",
       "123           stress    0.703067"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values(by='importance', ascending=False).head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f493f427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving features with non-zero coefficients\n",
    "nonzero_features = df.loc[df['importance'] > 0,]\n",
    "\n",
    "binary_classification_model_data = {\n",
    "    \"X_train\": token_counts_train_df2[nonzero_features['feature']],\n",
    "    \"X_test\": token_counts_test_df2[nonzero_features['feature']],\n",
    "    \"y_train\": y_train2,\n",
    "    \"y_test\": y_test2\n",
    "}\n",
    "\n",
    "# zapis do pickle\n",
    "with open(\"outputs/binary_classification_model_data.pkl\", \"wb\") as f:\n",
    "    pickle.dump(binary_classification_model_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c87c5685",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\Desktop\\WNE\\studia magisterskie\\2025-26\\SENTIMENT ANALYSIS Autumn 2025\\06\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'clf__C': 1, 'feature_select__top_n': 75}\n",
      "Best CV ROC AUC (OVR): 0.7041\n",
      "Test ROC AUC (OVR): 0.5469\n"
     ]
    }
   ],
   "source": [
    "# custom transformer to select top-n features based on predefined order\n",
    "class TopFeatureSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, feature_order, top_n=25):\n",
    "        self.feature_order = feature_order\n",
    "        self.top_n = top_n\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        selected = list(islice(self.feature_order, self.top_n))\n",
    "        return X[selected]\n",
    "\n",
    "# predefined feature order based on importance\n",
    "feature_order = list(mis_sorted_by_values3.keys())\n",
    "\n",
    "# pipeline: feature selection -> standardization -> LASSO logistic regression (multiclass)\n",
    "pipeline = Pipeline([\n",
    "    (\"feature_select\", TopFeatureSelector(feature_order=feature_order)),\n",
    "    (\"scale\", StandardScaler(with_mean=False)),\n",
    "    (\"clf\", LogisticRegression(\n",
    "        penalty=\"l1\",\n",
    "        solver=\"saga\",\n",
    "        multi_class=\"multinomial\",\n",
    "        random_state=42,\n",
    "        max_iter=5000\n",
    "    ))\n",
    "])\n",
    "\n",
    "# grid search parameters: number of features + regularization strength\n",
    "param_grid = {\n",
    "    \"feature_select__top_n\": [25, 50, 75, 100, 125, 150],\n",
    "    \"clf__C\": [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100, 1000, 10000, 100000]\n",
    "}\n",
    "\n",
    "# stratified k-fold cross-validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# grid search using multi-class ROC AUC (one-vs-rest average weighted with class frequency)\n",
    "grid = GridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    cv=cv,\n",
    "    scoring=\"roc_auc_ovr\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# fit the model on the training set\n",
    "grid.fit(token_counts_train_df3, y_train3)\n",
    "\n",
    "# print best hyperparameters and CV score\n",
    "print(\"Best params:\", grid.best_params_)\n",
    "print(\"Best CV ROC AUC (OVR):\", np.round(grid.best_score_,4))\n",
    "\n",
    "# evaluate on the test set\n",
    "y_test_pred_proba = grid.best_estimator_.predict_proba(token_counts_test_df3)\n",
    "test_auc_ovr = roc_auc_score(y_test3, y_test_pred_proba, multi_class=\"ovr\")\n",
    "print(\"Test ROC AUC (OVR):\", np.round(test_auc_ovr,4))\n",
    "\n",
    "# the whole procedure takes ~ min.\n",
    "# results are: AUC (train) = 0., AUC (test) = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cae23272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitted model\n",
    "best_model = grid.best_estimator_\n",
    "\n",
    "# coefficients from multinomial lasso regression\n",
    "coefficients = best_model.named_steps['clf'].coef_\n",
    "\n",
    "# compute importance as max abs coef across classes\n",
    "importance = np.max(np.abs(coefficients), axis=0)\n",
    "\n",
    "# dataframe with feature importance\n",
    "df = pd.DataFrame({\n",
    "    'feature': best_model.named_steps['feature_select'].feature_order[:best_model.named_steps['feature_select'].top_n],\n",
    "    'importance': importance\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c23d74cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>workplace</td>\n",
       "      <td>1.050524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>willing</td>\n",
       "      <td>1.010198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>get_yelled</td>\n",
       "      <td>0.903026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>team</td>\n",
       "      <td>0.823881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>outside</td>\n",
       "      <td>0.751657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>though</td>\n",
       "      <td>0.745254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>ever</td>\n",
       "      <td>0.727006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>made</td>\n",
       "      <td>0.707882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>love</td>\n",
       "      <td>0.707588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>along</td>\n",
       "      <td>0.689464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>work</td>\n",
       "      <td>0.621976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>workplace_get</td>\n",
       "      <td>0.613100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>care_employee</td>\n",
       "      <td>0.597937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>manager</td>\n",
       "      <td>0.581537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>time</td>\n",
       "      <td>0.574562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>low</td>\n",
       "      <td>0.562248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>company</td>\n",
       "      <td>0.557670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>care</td>\n",
       "      <td>0.548819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>employee_treated</td>\n",
       "      <td>0.543707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>yelled</td>\n",
       "      <td>0.537444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rude</td>\n",
       "      <td>0.533910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>decent</td>\n",
       "      <td>0.530115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>customer_rude</td>\n",
       "      <td>0.526750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>employed</td>\n",
       "      <td>0.525128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>two</td>\n",
       "      <td>0.521693</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             feature  importance\n",
       "38         workplace    1.050524\n",
       "68           willing    1.010198\n",
       "35        get_yelled    0.903026\n",
       "9               team    0.823881\n",
       "37           outside    0.751657\n",
       "51            though    0.745254\n",
       "46              ever    0.727006\n",
       "17              made    0.707882\n",
       "5               love    0.707588\n",
       "55             along    0.689464\n",
       "3               work    0.621976\n",
       "59     workplace_get    0.613100\n",
       "47     care_employee    0.597937\n",
       "11           manager    0.581537\n",
       "29              time    0.574562\n",
       "43               low    0.562248\n",
       "30           company    0.557670\n",
       "32              care    0.548819\n",
       "60  employee_treated    0.543707\n",
       "18            yelled    0.537444\n",
       "1               rude    0.533910\n",
       "26            decent    0.530115\n",
       "34     customer_rude    0.526750\n",
       "72          employed    0.525128\n",
       "52               two    0.521693"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values(by='importance', ascending=False).head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bbcbe81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving features with non-zero coefficients\n",
    "nonzero_features = df.loc[df['importance'] > 0,]\n",
    "\n",
    "multinomial_classification_model_data = {\n",
    "    \"X_train\": token_counts_train_df3[nonzero_features['feature']],\n",
    "    \"X_test\": token_counts_test_df3[nonzero_features['feature']],\n",
    "    \"y_train\": y_train3,\n",
    "    \"y_test\": y_test3\n",
    "}\n",
    "\n",
    "# zapis do pickle\n",
    "with open(\"outputs/multinomial_classification_model_data.pkl\", \"wb\") as f:\n",
    "    pickle.dump(multinomial_classification_model_data, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
